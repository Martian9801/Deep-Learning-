ADNAN SHAIKH

# AND GATES 
import numpy as np
np.set_printoptions(suppress=True) # TO avoid e^ values
# DECLARING PARAMETERS
p1 = np.array([0,0]) # FALSE
p2 = np.array([0,1]) # FLASE
p3 = np.array([1,0]) # FLASE
p4 = np.array([1,1]) # TRUE
w = np.array([0.2,0.3]) # RANDOM Initialization of Weights
lr = 1 # Learning rate
t1, t2, t3,t4 = 0, 0,0, 1 # AND GATE IS TURE IF BOTH INPUTS ARE HIGH

# Activation function (step function in this case)
def activation_function(net): 
    if net > 0: 
        return 1
    else:
        return 0

# Training
def train_perceptron(inputs, target, weights, learning_rate): # Function with input paramereter
    net = np.dot(weights, inputs)-0.3  # calculating w*input
    predicted_output = activation_function(net) # Activiation function call
    print(net,predicted_output)
    error = target - predicted_output # Calculating error if the predicted output is meeting target then error is zero
    new_weights = weights + learning_rate * error * inputs # If error is zero the second term will be zero so the weights are not updated
    new_weights = new_weights.astype(float) 
    return new_weights, error


print("Initial weights:", w)

counter=0
j=0
while(counter !=4):
    for i in range(1,5):
        if(i==1):
            p,t=p1,t1
        elif(i==2):
            p,t=p2,t2
        elif(i==3):
            p,t=p3,t3
        else:
            p,t=p4,t4
        w, err = train_perceptron(p,t, w, lr)
        print("Updated weights after iteration  :",j,'are', w)
        print("Error:", err)
        if(err==0):
            counter +=1
    if counter==4:
        print("Final Weights are",w)
        break
        
    else :
        counter=0
    j +=1



OUTPUT 
Initial weights: [0.2 0.3]
-0.3 0
Updated weights after iteration  : 0 are [0.2 0.3]
Error: 0
0.0 0
Updated weights after iteration  : 0 are [0.2 0.3]
Error: 0
-0.09999999999999998 0
Updated weights after iteration  : 0 are [0.2 0.3]
Error: 0
0.2 1
Updated weights after iteration  : 0 are [0.2 0.3]
Error: 0
Final Weights are [0.2 0.3]

    

    
